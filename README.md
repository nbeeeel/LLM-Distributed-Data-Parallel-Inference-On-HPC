# ğŸš€ Data Parallel Inference Framework

## ğŸŒŸ Overview
This project delivers a sophisticated data parallel inference framework leveraging PyTorch and the LLaMA 3.2-3B model, optimized for distributed processing across four GPU-enabled nodes. Integrated with SLURM for seamless job orchestration, it ensures high-throughput inference with minimal latency. The codebase is crafted for scalability, reliability, and elegance in high-performance computing environments.

## âœ¨ Features

- ğŸ–¥ï¸ Distributed Inference: Utilizes PyTorch's distributed module with NCCL backend for efficient GPU communication.
- âš™ï¸ SLURM Integration: Orchestrates jobs across nodes for optimal resource allocation.
- ğŸ’¾ Efficient Model Loading: Loads LLaMA 3.2-3B in FP16 precision to minimize memory usage.
- ğŸ“Š Dynamic Prompt Distribution: Automatically balances prompt workloads across nodes.
- ğŸ“ˆ Comprehensive Metrics: Tracks token generation speed, processing time, and GPU memory usage.

## ğŸ“ Repository Structure

- ğŸ“œ Data_Parallel_4_Nodes.py: Core script for data parallel inference with PyTorch and Transformers.
- ğŸ”§ Inference.slurm: SLURM script for configuring and running distributed inference tasks.
- ğŸ“– README.md: This file, your guide to the project.

## ğŸ› ï¸ Prerequisites

### Hardware:
- ğŸ–¥ï¸ 4 GPU-enabled nodes (NVIDIA GPUs with CUDA 12.2 support).
### Software:
- ğŸ Python 3.9+
- ğŸ”¥ PyTorch with CUDA support
- ğŸ¤— Hugging Face Transformers
- ğŸ“¡ SLURM workload manager
- ğŸ”— NCCL for distributed GPU communication

## Model:

ğŸ“‚ LLaMA 3.2-3B model weights at /mnt/lustre/user/llama3.2-3b-instruct.

## âš™ï¸ Setup Instructions

### Clone the Repository:

- git clone <repository-url>
- cd <repository-directory>

### Set Up Environment: 
Activate the Python environment and install dependencies:

- module load python/3.9 cuda/12.2
- source /mnt/lustre/user/llm-env/bin/activate
- pip install torch transformers

### Configure SLURM: 
Update NCCL_SOCKET_IFNAME in Inference.slurm to match your cluster's network interfaces.

### Prepare Model: 
Ensure LLaMA 3.2-3B model weights are available at the specified MODEL_PATH.

### ğŸš€ Running the Framework

- Launch the distributed inference job with SLURM:
- sbatch Inference.slurm

### What Happens?

- ğŸ”— Initializes the distributed environment with NCCL.
- ğŸ“¥ Loads the LLaMA 3.2-3B model on each node.
- ğŸ“¤ Distributes prompts across nodes for parallel processing.
- ğŸ“Š Collects and displays performance metrics.

## ğŸ“œ Output
Results are logged to:
- ğŸ“„ data_parallel_test_%j.out (output logs)
- âš ï¸ data_parallel_test_%j.err (error logs)
The master node (rank 0) aggregates results, displaying per-node and overall performance metrics, including token generation rates and processing times.

## âš¡ Performance Optimization

### ğŸ”Œ NCCL Configuration:
Tune NCCL_SOCKET_IFNAME and set NCCL_IB_DISABLE=1 for network compatibility.



ğŸ’¾ Memory Management: Uses FP16 to fit models on GPUs with â‰¥
