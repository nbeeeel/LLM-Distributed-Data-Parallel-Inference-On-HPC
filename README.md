# LLM-Distributed-Data-Parallel-Inference-On-HPC
Data parallel inference using PyTorch and LLaMA 3.2-3B model across 4 nodes with SLURM. Implements distributed GPU processing for efficient prompt handling.
